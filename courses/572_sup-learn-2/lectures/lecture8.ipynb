{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 572 lecture 8\n",
    "\n",
    "Big picture thoughts, random thoughts, wrap-up\n",
    "\n",
    "Announcements:\n",
    "\n",
    "- mid-program review update\n",
    "- capstone update\n",
    "- suggest going to the Zillow talk after this!\n",
    "\n",
    "Items from last time:\n",
    "\n",
    "- convolutions at boundary points\n",
    "- \"channels\" in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminology:\n",
    "\n",
    "- whitening\n",
    "- architecture search\n",
    "- Bayesian optimization / model-based optimization\n",
    "- reconstruction loss / reconstruction error\n",
    "- recurrent neural network (RNN)\n",
    "- transfer learning\n",
    "- GPU (graphics processing unit), GPGPU (general purpose GPU), CUDA\n",
    "- adversarial examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and initialization\n",
    "\n",
    "Preprocessing is how we handle the _data_; initialization refs to our initial guess at the _parameters_ before optimizing\n",
    "\n",
    "Preprocessing (about $X$ and $y$)\n",
    "\n",
    "- it helps to standardize the features (zero mean and variance=1)\n",
    "- some methods are not independent across features: can use PCA for preprocessing (see DSCI 563), other methods like [whitening](http://deeplearning.stanford.edu/wiki/index.php/Whitening)\n",
    "\n",
    "Initialization (about $W$)\n",
    "\n",
    "- sigmoids units are sensitive to initialization, ReLU less so\n",
    "- overall initialization matters _a lot_\n",
    "- cannot just initialize to zero because then gradient will be zero\n",
    "- Keras offers [a bunch of initialization options](https://keras.io/initializations/), you may need to experiment a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization\n",
    "\n",
    "We are left with a lot of hyperparameters. Hyperparameter optimization now becomes important. \n",
    "\n",
    "One important choice is the number of size/layers. Some prefer \"wide\" and others prefer \"deep\" nets. A specification of these is called the network _architecture_ and searching for the best one is sometimes called _architecture search_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "- do i need more data? try training on more or less data\n",
    "- did i not converge? try plotting training error vs iterations (book chapter 15)\n",
    "- did i overfit? try plotting train and test error with cross-validation\n",
    "- did i underfit? see how small i can make training error, use bigger method\n",
    "- was it an initialization problem?\n",
    "- a data preprocessing problem?\n",
    "- did i reach a local minimum?\n",
    "- have one where doing regression and output is sigmoid or tanh\n",
    "- did i overfit on the validation set?\n",
    "\n",
    "for each one... how to diagnose, what problems it will cause, relevant hyperparameters?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "10-minute break for teaching evaluation (this is strongly suggested by the Faculty of Science), just following orders.\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Autoencoders\n",
    "\n",
    "- unsupervised learning by minimizing _reconstruction loss_\n",
    "- we've usually associated loss functions with supervised learning, but actually several unsupervised methods like PCA and $k$-means clustering can be thought of us minimizing the reconstruction error\n",
    "- autoencoders are neural nets that try to reconstruct the input examples\n",
    "- squeeze the information through an \"information bottleneck\"\n",
    "- autoencoders -- relationship to PCA (DSCI 563) with one hidden layer and linear activations\n",
    "- thus you can also think of PCA as minimizing a squared error reconstruction loss\n",
    "- in fact various constraints on the compressed representation (hidden layer) lead to various algorithms we've already seen\n",
    "  - if you can only have one \"1\" and the rest \"0\" then that's $k$-means clustering / vector quantization\n",
    "- autoencoders automatically learn _compressed_ representations\n",
    "- strong connection between compression and unsupervised learning (especially the lossy part of compression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of neural networks\n",
    "\n",
    "- fully-connected (`Dense` in Keras)\n",
    "- convolutional\n",
    "- recurrent (incl. LSTM) - this is as opposed to feedforward (`Sequential` in Keras)\n",
    "- recursive (more general case of recurrent, acronym \"RNN\" usually refers to recurrent)\n",
    "- GAN\n",
    "- others..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strengths/weaknesses\n",
    "\n",
    "- need lots of data\n",
    "- Uncertainties / Bayesian \n",
    "- RNNs to be discussed in DSCI 575\n",
    "- Transfer learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPUs and parallel processing\n",
    "\n",
    "- breakdown of Moore's law\n",
    "- what parts are parallelizable?\n",
    "- matrix multiplication complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI safety and adversarial examples\n",
    "\n",
    "- If the input is a $1000\\times 1000$ image, the input space is so big\n",
    "  - it is hard to image all the possible perturbations\n",
    "- Relationship to DSCI 541\n",
    "- [OpenAI blog post](https://openai.com/blog/adversarial-example-research/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not covered in this course\n",
    "\n",
    "- A ton of stuff, of course, including...\n",
    "  - Reinforcement learning\n",
    "  - RNNs (DSCI 575)\n",
    "  - at-scale considerations (beyond DSCI 525)\n",
    "  - more...\n",
    "  \n",
    "\n",
    "For much more information, see the various resources on the [course README](https://github.ubc.ca/ubc-mds-2016/DSCI_572_sup-learn-2_students). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks!\n",
    "\n",
    "- This is my last lecture in the program. \n",
    "- Thanks for being so inquisitive and a great audience!\n",
    "- See you in DSCI 575 labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
