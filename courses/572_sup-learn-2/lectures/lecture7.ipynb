{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 572 Lecture 7\n",
    "\n",
    "convolutions and CNNs\n",
    "\n",
    "Terminology:\n",
    "\n",
    "- convolution\n",
    "- filter\n",
    "- convolutional neural network / convolutional network / convolutional net / convnet\n",
    "- fully connected\n",
    "- pooling\n",
    "- stride\n",
    "- object recognition, object detection, object localization\n",
    "- AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutions\n",
    "\n",
    "- What is a convolution?\n",
    "- What is a linear operator/transformation?\n",
    "  - in the discrete world, anything that can be represented as a matrix multiplication\n",
    "  - we have linear operatiors in the continuous world (like sum, derivative; hence the trick in the last lecture), but that is out of scope here\n",
    "  - finite differencing is a linear operator. PCA is linear. \n",
    "- 1-D convolutions\n",
    "- 2-D and higher-D convolutions\n",
    "- boundary conditions: the output might be slightly bigger or slightly smaller, or the same\n",
    "  - these are the options in [scipy.signal.convolve2d](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.signal.convolve2d.html)\n",
    "\n",
    "- (optional) all of this extends to continuous scenarios, where convolution is an integral instead of a sum\n",
    "- convolution is often written as $x \\ast y$ \n",
    "- convolution is commutative: $x \\ast y = y \\ast x$\n",
    "  - Nonetheless, we often have a \"signal\" and a \"filter\" and they have different interpretations\n",
    "  - The filter is often small and has an interpretation like \"highpass\" or \"lowpass\"\n",
    "\n",
    "- Note (for completeness, not required): there is a fast implementations of convolution using the FFT (fast Fourier transform). For an $n\\times n$ image and $m\\times m$ filter this takes $nm\\log(nm)$ time instead of $n^2m^2$. For sufficiently big images and filters this is a big win BUT there's a catch, which is that the equivalence only holds for periodic boundary conditions. \n",
    "\n",
    "- (Jump to lab and look at first convolution example, discuss. )\n",
    "\n",
    "- Convolutions appear all over the field of _signal processing_, which is traditionally a discipline within electrical engineering. But they also show up in math, physics, CS, etc. A lot of communications theory is based on this stuff.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What does this have to do with deep learning\n",
    "\n",
    "- Imagine an neural net with image as inputs. For a $1000\\times 1000$ image that is 1 million features.\n",
    "- Now say the next layer is 10% or even 1% that size. Now we need a matrix of size $10^6\\times10^4=10^{10}$. That is not going to happen.\n",
    "- Key insight: things happen \"locally\" in images. The top-left matters and the bottom-right matters, but they don't necessarily need to interact right away.\n",
    "  - so we do some \"local processing\" on the different parts of the image, and then \"report back\" and start merging the information when we've reduced the dimension\n",
    "  - this is the promise/dream of \"deep learning\": hierarchical abstractions like edges, curves, objects, higher and higher level \"understanding\"\n",
    "- Key idea: use layers that are not fully connected (this was called \"Dense\" in Keras). Instead, have units in layer 2 that only get input from some _nearby units_ (pixels) in layer 1. \n",
    "- The above notion is precisely a convolution. Thus people talk about convolutions but keep in mind it's just a not-fully-connected neural network. This means everything from before (gradients, tricks) carry over nicely. \n",
    "- But for computational reasons we don't form those giant matrices full of zeros! We just do convolutions. \n",
    "- The parameters (weights) are now the filters themselves. So we can interpret it as \"learning the filters\". It's all the same stuff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it all works\n",
    "\n",
    "- Let's say we start with an $m\\times m$ image.\n",
    "- Let's say we have $k^{(1)}$ filters in the first layers, and each is of size $3\\times 3$. \n",
    "- Then, we now have $k$ of these (approximately) $m\\times m$ images\n",
    "- We then _sum_ over these $k$ images to get back to one image\n",
    "  - You can think of this as a 3-d convolution, but it's not necessarily a helpful way of thinking\n",
    "- Next, we often apply some sort of _pooling_ which _decreases the size_ of the representation\n",
    "  - this is a downsampling operation \n",
    "  - A common approach is max pooling, which takes the maximum, say, a region of $2\\times 2$ pixels\n",
    "  - The intuition has to do with invariances (don't care if it's shifted a bit, just want maximum signal)\n",
    "  - There are other forms of pooling, like average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameters\n",
    "\n",
    "There are a lot of hyperparameters. The big ones are \n",
    "\n",
    "- _number of filters at each layer_ and _filter size_, which replace the layer size in regular fully-connected networks. Then there are pooling hyperparemeters. \n",
    "- There's also the _stride_ of the filters. For a regular convolution this is 1, but you may wish to slide the window more than 1 pixel each time (for example by the filter size for non-overlapping windows)\n",
    "- We retain all the usual hyperparamters of activation function, regularization, initialization, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing filters\n",
    "\n",
    "- This is something people love to do\n",
    "- Often see something similar to [Gabor filters](https://en.wikipedia.org/wiki/Gabor_filter) from human visual system\n",
    "- See AlexNet paper linked below, Figure 3.\n",
    "- The low layers are often similar in a lot of models\n",
    "  - Maybe we only need to retrain later layers when transferring to different tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras example\n",
    "\n",
    "Take a look at lab code, discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Famous architectures\n",
    "\n",
    "- Historically: Neocognitron, LeNet, HMAX\n",
    "- More recently:\n",
    "  - [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (named after Alex Krizhevsky) set a standard in 2012.\n",
    "  - [Inception / GoogLeNet](https://arxiv.org/pdf/1409.4842.pdf) (pun on Google & LeNet), 2014, 22 layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sets\n",
    "\n",
    "- There tend to be trendy datasets in object recognition that evolve over time as the field proceeds. \n",
    "- For example see this [compilation of MNIST results](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)\n",
    "- This is good because people can compare with each other\n",
    "- But also bad because these data sets may not be important and we may overfit on them (optimization bias as a whole field of researchers)\n",
    "  - there have been [disturbing findings](http://people.csail.mit.edu/torralba/research/bias/) about the limited transferability of models/insights to new data sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things people do\n",
    "\n",
    "- object detection\n",
    "- object recognition\n",
    "- image segmentation\n",
    "- art (see links on course README)\n",
    "- more recently, [GANs](https://en.wikipedia.org/wiki/Generative_adversarial_networks) for realistic image generation, see e.g. figure 2 of [this paper](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## More info / another look\n",
    "\n",
    "See CPSC 340 slides [Neural networks](https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L29.pdf), [Deep learning](https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L30.pdf), [CNNs](https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L31.pdf), [More CNNs](https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L32.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
